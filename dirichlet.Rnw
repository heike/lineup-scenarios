\documentclass[12pt]{article}
% \usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
% \geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\usepackage{graphicx}
%\usepackage{xcolor} % creates option clash, but still works
\usepackage[font=small,skip=5pt]{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
% \usepackage{color}
\usepackage{multirow}
\usepackage{rotating}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\usepackage{hyperref}
\graphicspath{{figure/}}
% \usepackage{endfloat} % Figures to the end of the document

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%---------------------------------------------------
%                 Editing Commands
\newcommand{\hh}[1]{{\color{magenta} #1}}
\newcommand{\cgj}[1]{{\color{orange} #1}}
\newcommand{\dc}[1]{{\color{cyan} #1}}

%---------------------------------------------------
%                 Placing Figures


%---------------------------------------------------
% Define new environment
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}

\newtheorem{dfkT}[theorem]{Definition}
\newenvironment{dfn} {\begin{dfkT} \rm\hfill\\}{\end{dfkT}}

%---------------------------------------------------

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Estimating $P$-values for the Lineup Protocol, to Determine Significance of Structure in Data Plots}
  \author{Heike Hofmann, Christian R\"ottger, Dianne Cook \thanks{
    The authors gratefully acknowledge funding from the National Science Foundation Grant \# DMS 1007697. All data collection has been conducted with approval from the Institutional Review Board IRB 10-347}\hspace{.2cm}\\
    Department of Statistics and Statistical Laboratory, Iowa State University\\
    and \\
    \\
    Department of Mathematics, Iowa State University\\
    and \\
    \\
    Department of Econometrics and Business Statistics, Monash University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Estimating $P$-values for the Lineup Protocol, to Determine Significance of Structure in Data Plots}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
Graphics play a crucial role in statistical analysis and data mining. Being able to quantify structure in data that is visible in plots, and how people read the structure from plots is an ongoing challenge. \hh{XXX just a placeholder}
\end{abstract}

\noindent%
{\it Keywords:}  Visual inference, Lineup protocol, \hh{XXX Other keywords?}.
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\tableofcontents
\newpage
\hh{todo-list
\begin{itemize}
\item discuss how multiple selections fit into the framework
\item does the distribution for the number of data detections extend to a non-integer number of picks (because of the weights)?
\item intro needs expansion: start with general intro on visual inference
\item re-analyze previous p-values and compare to metrics
\end{itemize}}

<<setup, echo = FALSE, message = FALSE, warning = FALSE>>=
options(replace.assign=TRUE,width=70, digits=2)
require(knitr)
opts_chunk$set(fig.path='figure/', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, par=TRUE, cache=FALSE, concordance=TRUE, autodep=TRUE, message=F, warning=F)

library(tidyr)
library(dplyr)
library(ggplot2)
library(ggtern)
library(xtable)
@

<<function, echo=FALSE>>=
alpha.ml <- function(p, weight=NULL, eps=10^(-7)) {
  # matrix p
  if (is.null(dim(p))) {
    m <- length(p)
    n <- 1
    p <- matrix(p, nrow=1)
  } else {
    n <- dim(p)[1]
    m <- dim(p)[2]
  }
  if (is.null(weight)) weight <- rep(1, n)
  else n <- sum(weight)

  weight <- weight/sum(weight)
  ps <- p
  ps[p < eps] <- eps # make sure we don't take the log of 0
  ps <- ps/rowSums(ps)
  logp <- sum(weight*rowSums(log(ps)))/m
  ml <- function(alpha) {
    digamma(alpha) - digamma(alpha*m) - logp
  }
  # find alpha such that digamma(alpha) - digamma(alpha*m) = logp
  alpha <- uniroot(f=ml, interval=c(eps,10))

  alpha$root
}
@

\section{Introduction}

Exploratory data analysis (EDA), an important component of data science (DS), today, relies heavily on data visualization, which traditionally, is not associated with the rigorous protocols of statistical inference. The lineup protocol introduced by \citep{buja} and validated in simulation settings by \citep{Mahbub} places data visualization into a framework of statistical inference, and thus provides a way to quantify the findings that are made by graphics. To imagine the importance of this, put yourself in the position of teaching new statisticians how to read residual plots to diagnose a linear model. With the lineup protocol, the residual plot can be furnished with an associated $p$-value that can tell you whether the pattern follows the model error assumptions. The lineup protocol allows us to furnish graphical findings with objective numerical values proportional to their significance similar to conventional hypothesis tests.

The lineup protocol requires that a data plot is shown as part of a page of plots, where the other plots on the page are produced assuming a null model. The process can be as simple as showing a scatterplot of two variables, embedded among null scatterplots produced by permuting one of the variables. Any `real' association between the two variables has been broken by the permutation, so if the data plot stands out as being different from the null plots, we would have evidence to say that the two variables are associated. `Associated' may take many forms, e.g. linear relationship, an outlier or few, clustering, nonlinear relation, and the beauty of the lineup protocol is that this does not need to be specified ahead of time. The alternative hypothesis can encompass a variety of departures from a null, and thus support graphics as a part of exploring data.

The lineup protocol has been used in a variety of ways to date. \citet{yin:2013} use it to evaluate if there is any structure in the results from a high-throughput RNA-Seq analysis experiment. \citet{loy:2015} describes the use of the lineup protocol to read residual plots from a hierarchical linear model, actually to show that the typical residual plot is misleading when trying to assess normality. \citet{lin:2014} use the lineup protocol with longitudinal data to explore cluster patterns.

\citet{Cheng:2013} utilize lineups to examine mountaineering.

\citet{Hullman:2015} discuss lineups in relation to Hypothetical Outcome Plots.

In a departure from EDA towards visual communication, \citet{hofmann:2012} describes computing power of a plot design, using the lineup protocol to determine the best way to communicate information visually.

\citet{zhao:2014}examined how people read lineups, using an eye-tracker. \citet{Healy:2014} summarizes the importance for sociology researchers.

ing the ‘interesting’ feature in the data is inserted randomly among a set of plots showing data that is consistent with a null hypothesis of ‘there is nothing interesting in the plot’. This null hypothesis is intentionally vague, in order for it to allow testing for a multitude of visual features. Obviously, being able to generate data that is consistent with the null hypothesis is crucial for
1
creating a lineup, and often times it is the null model that introduces some restrictions towards the features we can test.
In most experimental situations, lineups of size m = 20 are used. This size is convenient, because of its corresponding type I error rate (ie the probability of accidentally identifying the data plot, when guessing) of 1/20 = 0.05 based on a single evaluation. A major limiting factor in presenting lineups to participants in a study is mental fatigue, which sets in earlier for larger sized lineups because of the increased cognitive load. It is therefore of interest to investigate the option of using smaller sized lineups for studies. The smallest practical lineup size is three – ie a lineup consisting of two null plots and a randomly inserted data plot.
When drawing inference in the setting of a visual triangle test, 
\cgj{triangle? this sentence is about general lineups}
we have to further consider the exact procedure with which we collect information from  observers. In this paper, we want to distinguish between  three main scenarios:

\begin{itemize}
\item Scenario I\\
$K$ different lineups are shown to $K$ independent individuals.
%$X$ defined as the number of times an individual picked the data plot, follows under the null hypothesis that the observed data is consistent with the null data a Binomial distribution, with $X \sim B_{K, p=1/3}$.
\item Scenario II\\
Lineups consisting of the same data, but different null plots are shown to $K$ independent individuals.
\item Scenario III\\
The same lineup (same data, same nulls) is shown to $K$ independent individuals.
\end{itemize}
For scenario I, we need a model for generating a set of new `data' for each lineup; this scenario is therefore only practical in a simulation or sampling setting.

The distribution of the number of picks of the data plot depends on the specific scenario used in an experiment. To that effect we define a general {\it visual inference} distribution:
\begin{dfn}
Let $X$ be the number of picks of the data plot from a lineup of size $m$ evaluated by $K$ observers. The distribution of $X$ is denoted as the {\it visual inference distribution}
\[
X \sim V_{K, m, \alpha, s}
\]
where $\alpha$ is a concentration parameter with $\alpha > 0$, and
$s$ is the scenario ($s = I, II,$ or $III$) as defined above.
\end{dfn}
We will see in section~\ref{sec:binomial} that under scenario I the visual inference distribution is the same as the Binomial distribution, ie~$V_{K, m, 1} \equiv B_{K, 1/m}$ for any concentration parameter $\alpha>0$.
For scenarios II and III the number of picks of the data plot does not follow a binomial distribution, as can also be seen from the simulation results shown in table~\ref{tab:nobinom}, consisting of a side-by-side comparison of the probability to observe exactly $x$ data detections (out of 10 evaluations) under each of the three different scenarios. %For the purpose of this paper we will only consider $V$ distributions for lineups of size $m=20$.


\section{Rough thoughts on the model}


For a lineup of size $m$ let $X_i$ $i = 1, ..., m$ be the number of times that panel \#$i$ is identified as the most different out of $K$ evaluations (therefore $0 \le X_i \le K, \sum_i X_i = K$). Then the vector
 $X = (X_1, ..., X_m)$ is distributed according to a Multinomial distribution with probability vector $p = (p_1, ..., p_m)$, i.e.\ $0 \le p_i \le 1, \sum_i p_i = 1$. $p$ is therefore an element of the $(m-1)$-dimensional simplex $S(m-1)$.

We will assume that under the null distribution $p$ has a symmetric (also called \emph{flat}) Dirichlet distribution $Dir(\alpha)$, with $\alpha > 0$, i.e.\ all concentration rates $\alpha_i$ are identical.

The density of a flat $Dir(\alpha)$ is given as
\[
f(p_1, ..., p_{m-1}; \alpha) = \frac{\Gamma(\alpha m)}{\Gamma(\alpha)^m} \prod_{i=1}^{m}p_i^{\alpha-1}.
\]
%\cgj{Sorry to be so picky - checked this for alpha=1. Does not look like a density - perhaps need to multiply by volume of simplex? - XXX checked and looks OK}

When $\alpha=1$, the symmetric Dirichlet distribution is the uniform distribution on the $(m-1)$ dimensional simplex.
\cgj{In fact, for $\alpha = 1$ the density becomes a constant equal to ${(m-1)!}$. This is the inverse of the volume of the simplex $S(m-1)$.}
For $\alpha < 1$ the mass of the distribution is along the edges of the simplex, i.e.\ most values $p_i$ will be close to zero. For $\alpha > 1$ most values $p_i$ will be similar, with a mass in the middle of the simplex.
For all  Dirichlet distributions, the margins $p_i$ are distributed according to a Beta distribution, which in the case of a symmetric Dirichlet distribution simplify to $p_i \sim \text{Beta}\left(\alpha, (m-1)\alpha\right)$. In the symmetric case, we also know that $E[p_i] = 1/m$, i.e.\ the expected value of the probability to pick panel \#$i$ only depends on the size $m$ of the lineup.

\hh{XXX what meaning does the rate $\alpha$ have in the lineup setting? I would like to be able to say that $m\alpha$ is the number of 'interesting' looking null plots that we see on average in a lineup of size $m$. }

Figure~\ref{fig:simplex} shows examples of three Dirichlet distributions on $S(2)$ with different rates. Because we are dealing with a symmetric Dirichlet distribution, we can order each sample's values from highest to lowest. This forces all samples to appear in just one segment of the simplex (see Figure~\ref{fig:simplex.sorted}) and we can visualize each sample as a non-increasing line in a parallel coordinate plot where each of the margins represents one axis. Now the concentration parameter $\alpha$ corresponds to the rate at which these lines decrease: for $\alpha \to 0$ the Dirichlet distribution converges to a Dirac delta distribution, which in the sorted situation corresponds to $p(1) = 1$, while for  $\alpha to \infty$ all lines flatten out to a line in $1/m$.
Note that the visualization approach based on parallel coordinate plots is readily extensible for higher number of dimensions.

\begin{figure}
\begin{subfigure}[t]{\textwidth}
\caption{Examples of Dirichlet distributions on the 2-dimensional simplex.}
<<simplex, echo=FALSE, out.width='\\textwidth', fig.width=12, fig.height=4>>=
simplex <- c(1/3, 1, 3) %>% purrr::map_df(.f=function(alpha) {
  data.frame(gtools::rdirichlet(1000, alpha=rep(alpha, 3)))
}, .id="alpha")
names(simplex)[2:4] <- paste("p",1:3, sep="")
simplex$alphalabel <- factor(simplex$alpha)
levels(simplex$alphalabel) <- c("alpha: 1/3", "alpha: 1", "alpha: 3")

ggtern::ggtern(aes(x=p1, y=p2, z=p3), data=simplex) +
  geom_point() + facet_wrap(~alphalabel, labeller="label_parsed") +
  theme_bw() +
  theme(panel.background=element_rect(colour="black"))
@
\end{subfigure}
\begin{subfigure}[t]{\textwidth}
\caption{Corresponding marginal Beta($\alpha, 2\alpha$) densities.}
<<betas, echo=FALSE, out.width='\\textwidth', fig.width=12, fig.height=4>>=
p <- seq(0,1, by=0.005)
alpha <- c(1/3,1,3)
betas <- data.frame(expand.grid(p=p, alpha=alpha))
betas$density <- with(betas, dbeta(p, alpha, 2*alpha))
betas$density[is.infinite(betas$density)] <- 0
betas <- rbind(betas, data.frame(p=0,alpha=1,density=0))
betas$alphalabel <- factor(betas$alpha)
levels(betas$alphalabel) <- expression("alpha: 1/3", "alpha: 1", "alpha: 3")
qplot(data=betas, p, y=density, geom="polygon", fill=I("grey50"), alpha=I(.8), colour=I("grey20")) +
  ylab("density") + facet_wrap(~alphalabel, labeller="label_parsed") + ggplot2::theme_bw()
@
\end{subfigure}
\caption{\label{fig:simplex}Top row: Samples of 1000 points each from a $Dir(\alpha)$ distribution using different rate parameters $\alpha$.  Bottom row: marginal distributions of the Dirichlet distribution.}
\end{figure}


\begin{figure}
\begin{subfigure}[t]{\textwidth}
\caption{Ternary displays \citep{ggtern} \hh{XXX add citation to Graham's work} of dirichlet samples ordered from highest to lowest values. }
<<simplex-sorted, echo=FALSE, out.width='\\textwidth', fig.width=12, fig.height=4>>=
simplex$id <- 1:nrow(simplex)
dirichlets <- simplex %>%
  gather(key=margin, value=probs, starts_with("p"))

simplex.sorted <- dirichlets %>%
  group_by(alphalabel, id) %>%
  mutate(probs = sort(probs, decreasing=TRUE)) %>%
  spread(key=margin, value=probs)


ggtern::ggtern(aes(x=p1, y=p2, z=p3), data=simplex.sorted) +
  geom_point(alpha=I(.2)) + facet_wrap(~alphalabel, labeller="label_parsed") +
  theme_bw() +
  theme(panel.background=element_rect(colour="black"))
@
\end{subfigure}
\begin{subfigure}[t]{\textwidth}
\caption{Parallel Coordinate plots of ordered Dirichlet samples.}
<<parallelograms, echo=FALSE, out.width='\\textwidth', fig.width=12, fig.height=4>>=
dirichlets <- simplex.sorted %>%
  gather(key=margin, value=probs, starts_with("p"))

ggplot(aes(x=margin, y=probs), data=dirichlets) +
  geom_line(aes(group=id), alpha=0.1) +
  facet_wrap(~alphalabel) +
  ggplot2::theme_bw()
@
\end{subfigure}
\caption{\label{fig:simplex.sorted}Top row: Samples of 1000 points each from a $Dir(\alpha)$ distribution using different rate parameters $\alpha$.  Bottom row: marginal distributions of the Dirichlet distribution.}
\end{figure}




\subsection{Scenario III}

We are interested in the distribution of $X_m$, the number of times  that the data plot is identified in  $K$ independent evaluations.
We know that
$X_m \mid p_m \sim \text{Binom}_{K, p_m}$

\[
P(X_m = x \mid p_m) = {K \choose x} p_m^x (1-p_m)^{K-x}
\]


\begin{eqnarray}\nonumber
P(X_m = x) &=& E\left[ P(X_m = x \mid p_m) \right]   \\ \nonumber
&=& {K \choose x} \int_0^1  p_m^x (1-p_m)^{K-x} f(p_m) dp_m \\ \nonumber
&=& {K \choose x} \frac{1}{B\left(\alpha, (m-1)\alpha\right)} \int_0^1  p_m^x (1-p_m)^{K-x} p_m^{\alpha-1} (1-p_m)^{(m-1)\alpha-1} dp_m \\ \nonumber
&=& {K \choose x} \frac{B\left(x+\alpha, K-x + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)} \label{eq:vs3}
\end{eqnarray}
These probabilities are shown in Figure~\ref{fig:xm}. Depending on the rate $\alpha$ they are quite different from Binomial probabilities.
But for large values of $\alpha$ the densities converge in distribution to the Binomial with parameters $K$ and probability of success $p = 1/m$, because we can write $\Gamma(x + \alpha) = \prod_{i=0}^{x-1}(x + \alpha) \Gamma(\alpha)$ for any non-negative integer $x \ge 0$. Therefore the probability for  $x$ data detections becomes
\begin{eqnarray*}
P(X_m = x) &=& {K \choose x} \frac{\Gamma(m\alpha)}{\Gamma(K+m\alpha)} \cdot  \frac{\Gamma((m-1)\alpha + K-x)}{\Gamma((m-1)\alpha)} \cdot \frac{\Gamma(\alpha+x)}{\Gamma(\alpha)}  \\
&=& {K \choose x} \frac{\Gamma(m\alpha)}{\Gamma(m\alpha) \prod_{i=0}^{K-1} (i + m\alpha)} \cdot  \frac{\Gamma((m-1)\alpha) \prod_{i=0}^{K-x-1} ((m-1)\alpha + i)}{\Gamma((m-1)\alpha)} \cdot \\
&& \frac{\Gamma(\alpha) \prod_{i=0}^{x-1} (\alpha + i)}{\Gamma(\alpha)} \\
&=& {K \choose x} \frac{\prod_{i=0}^{K-x-1} ((m-1)\alpha + i) \prod_{i=0}^{x-1} (\alpha + i)}{\prod_{i=0}^{K-1} (i + m\alpha)} \\
&=& {K \choose x} \frac{\prod_{i=0}^{x-1} (\alpha + i)}{\prod_{i=0}^{x-1} (i + m\alpha)} \cdot
\frac{\prod_{i=0}^{K-x-1} ((m-1)\alpha + i) }{\prod_{i=x}^{K-1} (i + m\alpha)} \\
&=&  {K \choose x} \prod_{i=0}^{x-1} \frac{\alpha + i}{m\alpha + i} \cdot
\prod_{i=0}^{K-x-1} \frac{(m-1)\alpha + i}{ m\alpha + i+x }
\end{eqnarray*}
In the situation that the concentration parameter goes to infinity, the limiting distribution is therefore Binomial:
\begin{eqnarray*}
\lim_{\alpha \to \infty} P(X_m = x) &=& \lim_{\alpha \to \infty} {K \choose x} \prod_{i=0}^{x-1} \underbrace{\frac{\alpha + i}{m\alpha + i}}_{\to \frac{1}{m} \text{ for } \alpha \to \infty } \cdot
\prod_{i=0}^{K-x-1} \underbrace{\frac{(m-1)\alpha + i}{ m\alpha + i+x }}_{\to \frac{m-1}{m} \text{ for } \alpha \to \infty } \\
&=& {K \choose x} \left(\frac{1}{m}\right)^x \left(1 - \frac{1}{m}\right)^{K-x}
\end{eqnarray*}


The expected value is derived in section~\ref{sec:expected}.




\begin{figure}
<<xm, echo=FALSE, fig.width=8, fig.height=3, out.width='\\textwidth'>>=
Kxs <- c(5,10,15) %>% purrr::map_df(
  .f=function(K) {
    data.frame(x=0:K)
  }, .id="K")
Kxs$K <- c(5,10,15)[as.numeric(Kxs$K)]

xmDistr <- rbind(
  data.frame(alpha=1/3, Kxs),
  data.frame(alpha=1, Kxs),
  data.frame(alpha=3, Kxs))

xmDistr$prob <- with(xmDistr, choose(K,x)/beta(alpha, 19*alpha)*beta(x+alpha, K-x+19*alpha))
xmDistr$binom <- with(xmDistr, dbinom(x, size=K, prob=1/20))
xmDistr$alphalabel <- factor(xmDistr$alpha)
levels(xmDistr$alphalabel) <- c("1/3", "1", "3")
ggplot(data=xmDistr, aes(x=x, y=prob)) +
  facet_grid(.~K, scales="free_x", space="free_x",
             labeller="label_both") + theme_bw() +
  geom_segment(aes(x=x-.4, xend=x+.4, y=binom, yend=binom),
               colour="grey50") +
  geom_point(aes(colour=alphalabel)) +
  scale_colour_brewer("alpha", palette="Set2") +
  scale_x_continuous(breaks=0:15, minor_breaks=NULL) +
  ylab("P(X = x)") +
  xlab("Number of data detections x") +
  ggplot2::theme_bw()
@
\caption{\label{fig:xm} Probability of the number of data detection under the null hypothesis in $K$ evaluations. The grey line segments correspond to a binomial probability $B(K, 1/20)$.}
\end{figure}


\subsection{Scenario I}\label{sec:binomial}
Let us assume that the lineups in the study are administered such that each lineup is only shown to one participant. This is only practical, if we have a way to `produce' data, i.e. we are in a situation where we can either use sampling or create data from a model.
Then $X$, the number of times the data was identified from the lineup in $K$ independent evaluations, is given as the sum of Bernoulli variables $X_i$, such that
$X = \sum_{i=1}^K X_i$ with $X_i \mid p_i \sim \text{Binom}_{1, p_i}$.
Then the distribution of $X$ is a Binomial distribution with parameters $K$ and $1/m$, because the $X_i$ are independent and identical with $p_i = 1/m$:
\begin{eqnarray*}
P(X_i = k) &=&  \frac{1}{B(\alpha, (m-1)\alpha)} B(k + \alpha, 1- k + (m-1)\alpha) \\
&=& \begin{cases}
    \frac{B(\alpha, (m-1)\alpha+1)}{B(\alpha, (m-1)\alpha)} & \text{if } k = 0, \\
    \frac{B(\alpha+1, (m-1)\alpha)}{B(\alpha, (m-1)\alpha)}              & \text{if } k = 1
    \end{cases},\\
&=& \begin{cases}
    1- 1/m & \text{if } k = 0, \\
    1/m              & \text{if } k = 1
\end{cases}.
\end{eqnarray*}

\subsection{Scenario II}
Scenario II is the situation, where we keep the same data plot in the lineup, but exchange the null plots. The main idea in analyzing this scenario is to use the different sets of nulls for estimating one overall concentration parameter for $\alpha$ using an ML approach with weights according to the number of evaluations available for each lineup. We can then compute one overall $p$-value based on the number of data detection across all lineups. \hh{XXX there is a bit of a problem with dependence. The data detections come from a mix of different lineups, so there is a bit of a dependence/independence issue going on, that I'd still like to look into. XXX The overall p-value should not be very different from the individual $p$-values we get by assuming each of the lineups was done under scenario III. }

\section{Estimating the concentration parameter}

<<ex-elect, echo=FALSE>>=
# needs to be put into data folder - but please don't put this data onto the repo
raw11 <- read.csv("data/corrected_data_turk11.csv", stringsAsFactors = FALSE)

elect <- raw11 #[grep("electoral", as.character(raw11$param_value)),]
response <- strsplit(elect$response_no, split=",")
elect$weight <- response %>% purrr::map(.f = length) %>% unlist()
bigelect <- elect %>% purrr::map(.f = function(x) rep(x, elect$weight)) %>% data.frame()
bigelect$response_no <- unlist(response)
bigelect$weight <- 1/bigelect$weight

filler <- data.frame(expand.grid(
  param_value=unique(bigelect$param_value),
  response_no=1:20))
bigelect <- merge(bigelect, filler, by=c("param_value", "response_no"), all=TRUE)
bigelect$example <- gsub("(.*)-[1-5]","\\1", as.character(bigelect$param_value))
bigelect$weight[is.na(bigelect$weight)] <- 0
bigelect <- bigelect %>% group_by(param_value) %>%
  mutate(plot_location=na.omit(plot_location)[1])
bigelect$correct <- with(bigelect, !(response_no!=plot_location))

all <- bigelect
bigelect <- subset(bigelect, example=="electoral")

pars <- bigelect %>% filter(correct!=TRUE) %>% select(param_value, response_no, weight) %>%
  split(.$param_value, drop=TRUE) %>%
  purrr::map(.f = function(x) {
    x$id <- 1:nrow(x)
    dir <- tidyr::spread(x, key=response_no, value=weight, fill=0)
    sirt::dirichlet.mle(dir[,-(1:3)])
  })


pval3 <- function(x, K, alpha, m=20) {
  i <- seq(round(x), K)
  sum(choose(K, i)/beta(alpha, (m-1)*alpha) * beta(i + alpha, K-i+(m-1)*alpha))
}

ns <- bigelect %>%  group_by(param_value) %>% summarize(n=sum(weight), detected=sum(correct*weight))
ns$alpha <- pars %>% purrr::map(function(x) x$alpha0/19) %>% unlist()
ns$dirprob <- 1:nrow(ns) %>% purrr::map(.f=function(i) pval3(ns$detected[i], K=ns$n[i], alpha=ns$alpha[i])) %>% unlist()
@

<<ex-elect-table, dependson='ex-elect', echo=FALSE, results='asis'>>=
xtable(ns, digits=c(0,0,0, 1, 4,5))
@




\section{Derivations}

\subsection{Distribution of data detections in Scenario III under the null hypothesis}
The probability of the number of data detections $x$ under the null hypothesis is given as:
\[
P(X = x) = {K \choose x} \frac{1}{B\left(\alpha, (m-1)\alpha\right)} \cdot B\left(x+\alpha, K-x + (m-1)\alpha \right)
\]
To show that the above probabilities form a mass function for  $x = 0, ... K$ it helps to verify the following property of the Beta function for all positive real values $\alpha, \beta$:
\begin{eqnarray*}
\frac{B(\alpha + 1, \beta)}{B(\alpha, \beta)} = \frac{\alpha}{\alpha+\beta} \ \ \ \ \text { and } \ \ \ \
\frac{B(\alpha, \beta + 1)}{B(\alpha, \beta)} = \frac{\beta}{\alpha+\beta}.
\end{eqnarray*}

Let $X^{(K)}$ be the random variable describing the number of data detections in $K$ evaluations of a lineup of size $m$, then
for $K = 1$ the probabilities form a mass function:
\begin{eqnarray*}
P(X^{(1)} = 0) &=&  \frac{B\left(\alpha, 1 + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)} = \frac{(m-1)\alpha}{m\alpha}  = 1 - \frac{1}{m} \\
P(X^{(1)} = 1) &=& \frac{B\left(1+\alpha, (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)} = \frac{\alpha}{m\alpha} = \frac{1}{m}.
\end{eqnarray*}
We can prove that the probabilities form a probability mass function for general $K$.
\begin{eqnarray*}
\sum_{x = 0}^K P(X^{(K)} = x) &=& \sum_{x = 0}^K {K \choose x} \frac{B\left(x+\alpha, K-x + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)}  \\
&=& \frac{B\left(\alpha, K + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)} +
\frac{B\left(K+\alpha, (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)}  + \\
&& \sum_{x = 1}^{K-1} {K \choose x} \frac{B\left(x+\alpha, K-x + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)}  \\
&=&  \frac{B\left(\alpha, K + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)} + \sum_{x = 1}^{K-1} {{K-1} \choose {x-1}} \frac{B\left(x+\alpha, K-x + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)} + \\
&& \frac{B\left(K+\alpha, (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)}  + \sum_{x = 1}^{K-1} {{K-1} \choose x} \frac{B\left(x+\alpha, K-x + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)} \\
&=& \sum_{x = 1}^{K} {{K-1} \choose {x-1}} \frac{B\left(x+\alpha, K-x + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)} + \\
&& \sum_{x = 0}^{K-1} {{K-1} \choose x} \frac{B\left(x+\alpha, K-x + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)} \\
&=& \sum_{x = 0}^{K-1} {{K-1} \choose {x}} \frac{B\left(x+1+\alpha, K-1-x + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)} + \\
&& \sum_{x = 0}^{K-1} {{K-1} \choose x} \frac{B\left(x+\alpha, K-1-x + (m-1)\alpha +1\right)}{B\left(\alpha, (m-1)\alpha\right)} \\
&=& \sum_{x = 0}^{K-1} P(X^{(K-1)} = x) \cdot \frac{B\left(x+1+\alpha, K-1-x + (m-1)\alpha \right)}{B\left(x+\alpha, K-1-x + (m-1)\alpha \right)} + \\
&& \sum_{x = 0}^{K-1} P(X^{(K-1)} = x) \cdot  \frac{B\left(x+\alpha, K-1-x + (m-1)\alpha +1\right)}{B\left(x+\alpha, K-1-x + (m-1)\alpha \right)} \\
&=& \sum_{x = 0}^{K-1} P(X^{(K-1)} = x) \cdot \frac{x+\alpha}{K-1+m\alpha} + \\
&& \sum_{x = 0}^{K-1} P(X^{(K-1)} = x) \cdot  \frac{K-1-x+(m-1)\alpha}{K-1+m\alpha} =  \sum_{x = 0}^{K-1} P(X^{(K-1)} = x) \\
\end{eqnarray*}
By induction over $K$ therefore follows that $P(X^{(K)} = x)$ is a probability mass function for $x = 0, ..., K$ for all positive integers $K$.
% \begin{eqnarray*}
% P(X^{(1)} = 0) &=& \frac{B\left(\alpha, 1 + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)} \\
% &=& \frac{\Gamma(\alpha) \Gamma(1 + (m-1)\alpha) \Gamma(m\alpha)}{ \Gamma(\alpha + 1 + (m-1)\alpha ) \Gamma(\alpha) \Gamma((m-1)\alpha)}
% \begin{eqnarray*}

\subsection{Expected value of the number of data detection in scenario III under the null hypothesis}\label{sec:expected}
For the expected number of  data detections in $K$ evaluations of scenario III under the null hypothesis we get the following recursive expression:
\begin{eqnarray*}
E[X^{(K)}] &=& \sum_{x = 0}^K x P(X{(K)} = x) = \sum_{x = 1}^K x P(X{(K)} = x) \\
&=&  \sum_{x = 1}^K x {K \choose x} \frac{B\left(x+\alpha, K-x + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)}  \\
&=& K \sum_{x = 1}^K {{K -1} \choose {x-1}} \frac{B\left(x+\alpha, K-x + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)} \\
&=& K \cdot \sum_{x = 0}^{K-1} {{K-1} \choose {x}} \frac{B\left(x+\alpha+1, K-1-x + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)} \\
&=& K \cdot \sum_{x = 0}^{K-1}  P(X^{(k-1)} = x) \frac{x+\alpha}{K-1+m\alpha} \\
&=& \frac{K}{K-1+m\alpha} \left(E[X^{(K-1)}] + \alpha\right). \\
\end{eqnarray*}
For $K=1$ we have an expected value of $E[X^{(1)}] = \frac{m-1}{m}$.

\subsection{Estimation of the concentration parameter}

Let $P_1, ..., P_n$ be the vectors of observed picking probabilities for $n$ lineups, i.e. for  lineup \#$i$ we have a vector $P_i = (p_1, ..., p_{m_o})$ of probabilities for picking null panel $j = 1, ..., m_o$ in the lineup, where ${m_o}$ is the number of null plots in the lineup of size $m$.

Assuming a symmetric Dirichlet distribution we estimate the concentration parameter $\alpha$ using a maximum likelihood approach as:
\begin{eqnarray*}
{\cal L}(\alpha; P_1, ..., P_n) &=& \prod_{j=1}^n
\frac{\Gamma(\alpha m_o)}{\Gamma(\alpha)^{m_o}} \prod_{i=1}^{m_o} p_{ij}^{\alpha-1} = \frac{\Gamma(\alpha {m_o})^n}{\Gamma(\alpha)^{m_on}} \prod_{i=1}^n \prod_{i=1}^{m_o} p_{ij}^{\alpha-1}
\end{eqnarray*}

The corresponding log likelihood function is then
\begin{eqnarray*}
\log {\cal L}(\alpha; P_1, ..., P_n) &=& n \log \Gamma(\alpha {m_o}) - n{m_o} \log \Gamma(\alpha) + (\alpha-1) \sum_{i,j} \log p_{ij}
\end{eqnarray*}
Let $\psi(x)$ denote the \emph{digamma} function. Digamma is defined as the derivative of the log Gamma function, i.e.
\[
\psi(x) = \frac{\partial}{\partial x} \log{\Gamma(x)} = \frac{\Gamma'(x)}{\Gamma(x)}
\]
With this definition the maximum likelihood estimate $\hat{\alpha}$ of the concentration parameter $\alpha$ is the solution to
\[
\label{eq:ml1} \tag{ML1}
\psi(\alpha) - \psi({m_o}\alpha) = \frac{1}{{m_o}n} \sum_{i,j} \log p_{ij}
\]

There are several advantages to estimating a single concentration parameter as opposed to averaging across a vector of concentration parameters $\alpha_i$, $i = 1, ..., m_o$, (ML2) as proposed by \citet{minka}. Most importantly, the ML estimate of a single concentration parameter is less biased than the estimate based on the average of $\alpha_i$, as can be seen in Figure~\ref{fig:mls}.
Another advantage of estimating a single concentration parameter directly, is that the estimate can be based on a single instance (such as we have for evaluations from just one lineup). 

 \hh{XXX look into dirmult package}
\begin{figure}
<<echo=FALSE, fig.width=7, fig.height=4, out.width='.75\\textwidth'>>=
set.seed(20140501)
alpha <- 0.3
#Beta <- function(alpha) {prod(gamma(alpha))/gamma(sum(alpha)) } 
#pdirichlet <- function(x, alpha) { 1/Beta(alpha)*prod(x^(alpha-1))}

res <- 1:1000 %>% purrr::map_df(function(i) {
  ps <- gtools::rdirichlet(5, alpha=rep(alpha, 20))
  data.frame(ML1=alpha.ml(p=ps), ML2=sirt::dirichlet.mle(ps, eps=10^-7)$alpha0/20)
})
res$ids <- 1:nrow(res)
res2 <- tidyr::gather(res, key=method, value=value, -ids)
qplot(value, geom="density", data=res2, 
      fill=I("grey70"), alpha=I(.5), colour=I("grey50"))  + 
  geom_vline(aes(xintercept=alpha), colour="grey30", size=.5) +
  geom_text(x=alpha, y = 12.5, label=expression("alpha=",alpha), 
            colour="grey30", size=5, data=data.frame()) +
  facet_grid(.~method) + ggplot2::theme_bw() + 
  xlab("Estimates of concentration parameter") + 
  scale_x_continuous(breaks=c(alpha, c(0,2,4,6)/10), 
                     labels=c(expression(alpha), c(0,2,4,6)/10))

# summary(res)
@
\caption{\label{fig:mls} Comparison of estimates of concentration parameter $\alpha$ using methods ML1 (direct estimation) and ML2 (average of $\hat{\alpha}_i, i = 1, ..., m_o$) based on five instances of P each.}
\end{figure}


\begin{figure}
<<density-1p, echo=FALSE, fig.width=6, fig.height=4, out.width='.5\\textwidth'>>=
set.seed(20140501)
alpha <- 0.3
res <- 1:1000 %>% purrr::map(function(i) {
  ps <- gtools::rdirichlet(1, alpha=rep(alpha, 20))
#  sirt::dirichlet.simul( alpha=matrix(rep(alpha, 1*20), ncol=20) )
  alpha.ml(p=ps)
}) %>% unlist
qplot(res, geom="density", fill=I("grey70"), alpha=I(.5), colour=I("grey50"))  + 
  geom_vline(aes(xintercept=alpha), colour="grey30", size=1) +
  ggplot2::theme_bw() +
    xlab("Estimates of concentration parameters") + 
  scale_x_continuous(breaks=c(alpha, c(0,2,4,6,8)/10), 
                     labels=c(expression(alpha), c(0,2,4,6,8)/10))

#' library(fitdistrplus)
#' distr <- fitdist(res, "lnorm")
#' plot(distr)
#summary(res)
@
\caption{Density plots of ML1 estimates of the concentration parameter $\alpha$ based on 1,000 single instances of $P$. }
\end{figure}

\subsubsection*{Variation of the estimation in the presence of weights}
Assume that $P_1, ..., P_n$ are observed $w_1, ..., w_n$ number of times, where $w_j \ge 0$  for all $j = 1, ..., n$.
The likelihood function then changes to

\begin{eqnarray*}
{\cal L}(\alpha; P_1, ..., P_n) &=& \frac{\Gamma(\alpha {m_o})^{\sum_j w_j}}{\Gamma(\alpha)^{m_o\sum_j w_j}} \prod_{j=1}^n \prod_{i=1}^{m_o} p_{ij}^{{w_j}(\alpha-1)}
\end{eqnarray*}
Let $w$ be the sum of the weights $w = \sum_j w_j$, then for the log likelihood using weights we get
\begin{eqnarray*}
\log {\cal L}(\alpha; P_1, ..., P_n) &=&  w \log \Gamma(\alpha {m_o}) - w {m_o} \log \Gamma(\alpha) + (\alpha-1) \sum_{i,j} w_j \log p_{ij}
\end{eqnarray*}

The ML estimate of the concentration parameter $\alpha$ then becomes the solution of the weighted expression
\[
\psi(\alpha) - \psi({m_o}\alpha) = \frac{1}{{m_o} w} \sum_{i,j} w_j \log p_{ij}.
\]
What is obvious from the expression is that the concentration parameter is independent of the overall sum of the weights $w$, and only relative differences between the $w_j$ are regarded.

\subsection{Summary of all concentration parameters and $p$-values}
An overview of all of the concentration parameters estimated from our lineup experiments are shown in Figure~\ref{fig:comparisons}.

<<pvals,echo=FALSE, results='asis'>>=
ns <- all %>% split(.$param_value) %>% purrr::map_df(
  function(x) {
    nulls <- subset(x, !correct)
    alpha = alpha.ml(as.vector(xtabs(weight~response_no, data=nulls))/sum(nulls$weight))
    data.frame(
      K=sum(x$weight),
      x=sum(subset(x,correct)$weight),
      alpha=alpha,
      pval=pval3(
        x=sum(subset(x,correct)$weight),
        K=sum(x$weight),
        alpha=alpha
                 )
      )
  }, .id="param_value")

ns$example <- gsub("(.*)-[1-6]","\\1",as.character(ns$param_value))

xpls <- all %>% split(.$example) %>% purrr::map_df(
  function(x) {
    nulls <- subset(x, !correct)
    lps <- xtabs(weight~factor(param_value)+response_no, data=nulls)
    alpha = alpha.ml(lps/rowSums(lps))
    data.frame(
#      K=sum(x$weight),
#      x=sum(subset(x,correct)$weight),
      alpha.overall=alpha,
      pval.overall=pval3(
        x=sum(subset(x,correct)$weight),
        K=sum(x$weight),
        alpha=alpha
                 )
      )
  }, .id="example")
nsall <- merge(ns, xpls, by="example")
@


<<tab-alpha, echo=FALSE, results='hide', dependson='pvals'>>=
xtable(nsall[,-1], digits=c(0,0,0,1,4,4,4,4))
@

\begin{figure}
\centering
\begin{subfigure}[t]{.45\linewidth}
\caption{Concentration parameters}
<<fig-alpha, echo=FALSE, dependson='pvals', out.width='\\linewidth'>>=
nsall$example <- reorder(nsall$example, nsall$alpha)
qplot(example, alpha, data=nsall) +
  ggplot2::theme_bw() + expand_limits(y=c(0,NA)) +
  geom_point(aes(y=alpha.overall), colour="orange", pch="x", size=5) +
  coord_flip() + xlab("") +
  ylab("Concentration parameter alpha")
@
\end{subfigure}
\begin{subfigure}[t]{.45\linewidth}
\caption{Visual $p$-values}
<<fig-pvals, echo=FALSE, dependson='pvals', out.width='\\linewidth'>>=
#ns$example <- reorder(ns$example, ns$alpha)
qplot(example, pval, data=nsall, geom="jitter", width=0.2) +
  ggplot2::theme_bw() + expand_limits(y=c(0,NA)) +
  geom_point(aes(y=pval.overall), colour="orange", pch="x", size=5) +
  coord_flip() + xlab("") +
  ylab("P-value")
@
\end{subfigure}
\caption{\label{fig:comparisons} Summaries of concentration parameters and $p$-values based on individual lineups (dark circles) and overall values for each of the examples (colored crosses).}
\end{figure}

All of the concentration parameters are centered around similar values. 
Figure~\ref{fig:lognormal} shows a density plot of all estimates for the concentration parameter overlaid by a blue line showing the fit of a lognormal density \citep[using the {\tt fitdistrplus} package][]{fitdistrplus}. 

\begin{figure}
<<concentration-lognormal,results='asis', echo=FALSE, fig.width=5, fig.height=3, out.width='.5\\linewidth'>>=

library(fitdistrplus)
distr <- fitdist(nsall$alpha, "lnorm")
distrfit <- data.frame(x = seq(0, .7, length=100))
distrfit$fit <- dlnorm(distrfit$x, meanlog = distr$estimate[1], sdlog = distr$estimate[2])

ggplot(data=nsall) + geom_density(aes(x=alpha), fill="grey75", bw=0.0425) + ggplot2::theme_bw() +
  geom_line(aes(x=x, y=fit), data=distrfit, colour="steelblue")
#plot(distr)
@
\caption{\label{fig:lognormal}Density plot of the estimates of the concentration parameter $\alpha$ of all of the lineup experiments. The blue line overlaid is the density of a log normal distribution fit to the same values. }
\end{figure}

Instead of a single value for the concentration parameter $\alpha$ we can use a lognormal distribution with hyper parameters $\mu$ and $\sigma$ and estimate the probability for success based on this distribution. For our study $\hat{\mu} =$\Sexpr{distr$estimate[1]} and $\hat{\sigma} =$\Sexpr{distr$estimate[2]}, which puts the median of the estimated lognormal distribution at \Sexpr{exp(distr$estimate[1])} and its mean at \Sexpr{exp(distr$estimate[1]+.5*distr$estimate[2]^2)}.

Let us think of $\alpha$ as a random variable with a lognormal distribution, we can compute the probability of $X \ge x$ as 
\begin{eqnarray*}
P(X \ge x) &=& E \left[ P(X \ge x \mid \alpha)\right] \\
&=& \int_0^\infty P(X \ge x \mid \alpha) f(\alpha) d\alpha,
\end{eqnarray*}
where $f(\alpha)$ is the density of the lognormal distribution:
\[
f(\alpha) = \frac{1}{\alpha \sigma_\alpha \sqrt{2\pi}} \exp \left[ - \frac{\log\alpha - \mu_\alpha}{2\sigma_\alpha^2}\right]. 
\]
For $\mu_\alpha$ and $\sigma_\alpha$ we use the estimates from the overall study as give above. However, the results are not particularly sensitive to the exact values of these hyperparameters. Figure~\ref{fig:hyper} gives a

<<bayes, echo=FALSE>>=
bayes <- function(x, K, m, mu = distr$estimate[1], sigma = distr$estimate[2]) {
  alpha <- seq(10^-6, 3, by=0.005)
#  alphaprobs <- alpha %>% purrr::map(.f=dlnorm, meanlog = distr$estimate[1], sdlog = distr$estimate[2]) %>% unlist()
    alphaprobs <- alpha %>% purrr::map(.f=dlnorm, meanlog = mu, sdlog = sigma) %>% unlist()
  probs <- alpha %>% purrr::map(.f=pval3, x=x, K=K, m=m) %>% unlist()
  sum(probs*alphaprobs)/sum(alphaprobs)
}

nsall$pval.bayes <- 1:nrow(nsall) %>% purrr::map(.f = function(i) {
  with(nsall, bayes(x=x[i], K=K[i], m=20))
  }) %>% unlist()

ggplot(data=nsall, aes(x = pval, pval.bayes)) + 
  geom_abline() +
  geom_point() + xlim(c(0, 0.1)) + ylim(c(0,0.1)) + 
  geom_text(aes(label = param_value), data=subset(nsall, abs(pval-pval.bayes) > 0.01))
@

\section{Empirical distribution of the number of null plot picks}

\hh{XXX what is the distribution of the p values for the number of picks of  null plots? It is not a uniform distribution, because we observe data for a minimums/maximums decision}

Let $X_i$ be the random variable describing the number of times that panel \#$i$ was picked in $K$ evaluations of the lineup.
Let $q_i$ be the corresponding probability to observe at least $x_i$ number of picks of panel \#$i$.
\[
  q_i = P(X_i \ge x_i) =  \sum_{k = x_i}^K {K \choose k} \frac{B\left(x_i+\alpha, K-x_i + (m-1)\alpha \right)}{B\left(\alpha, (m-1)\alpha\right)}
  \]
Note that for the data plot this corresponds to the visual $p$-value.

Because the question during the evaluation process is asking the participant to identify an extreme plot, typically  ``Which plot is the most different?", we are observing data from a minimums distribution:


<<probs-null, echo=FALSE, results='asis'>>=
nulls <- bigelect %>% filter(!correct) %>% group_by(param_value, response_no) %>%
summarize(
pick = sum(weight, na.rm=T)
)
nulls <- nulls %>% group_by(param_value) %>%
mutate(
n = sum(pick, na.rm=T)
)
nulls <- merge(nulls, ns[, c("param_value", "alpha")], by="param_value")

ns <- bigelect %>% group_by(param_value) %>% summarize(
  n=sum(weight),
  detected=sum(weight[response_no==plot_location])
)

# refit the dirichlet based on five means
nulls$perc <- with(nulls, pick/n)
null_means <- tidyr::spread(nulls[,c("param_value", "response_no", "perc")], response_no, perc, fill=0) # filling with 0 isn't quite right
one_par <- sirt::dirichlet.mle(null_means[,2:21], weight=ns$n-ns$detected)
one_par$alpha0/19
ns$alpha.ours <- 1:nrow(null_means) %>% purrr::map(function(i) alpha.ml(null_means[i,2:21])) %>% unlist
ns$alpha.one.ours <- alpha.ml(null_means[,2:21], weight=ns$n-ns$detected)

ns$dirprob2 <- 1:nrow(ns) %>%
purrr::map(function(i) pval3(ns$detected[i], K=ns$n[i], alpha=one_par$alpha0/19)) %>% unlist()
ns$pval <- 1:nrow(ns) %>%
purrr::map(function(i) pval3(ns$detected[i], K=ns$n[i], alpha=ns$alpha.ours[i])) %>% unlist()
ns$pval2 <- 1:nrow(ns) %>%
purrr::map(function(i) pval3(ns$detected[i], K=ns$n[i], alpha=ns$alpha.one.ours[i])) %>% unlist()
ns <- ns %>% select(-c(alpha, dirprob2))
xtable(ns, digits=c(0,0,0,1,4,4,4,4))
pval3(sum(ns$detected), K=sum(ns$n), alpha=ns$alpha.one.ours[1])
@

\[
P( \min_i X_i \le x) = 1 - (1-x)^{m-1}.
\]
We are really dealing with Beta distributed variables $X_i$. The resulting probabilities actually look more like a sample from a Beta distribution.


\begin{figure}
\centering
<<ex-summary, echo=FALSE, fig.width=8, fig.height=3, out.width='.8\\textwidth'>>=
ps <- pars %>% purrr::map(.f = function(x) x$xsi) %>% data.frame()
alphas <- tidyr::gather(ps, key=lineup, value=prob)

ps2 <- pars %>% purrr::map(.f = function(x) names(x$xsi)) %>% data.frame()
names <- tidyr::gather(ps2, key=lineup, value=name)
alphas$response_no <- names$name
# not sure how the xsi relate to the probabilities ...
qplot(param_value, perc, data=nulls) + theme_bw() + ylab("Probabilities p") +xlab("") +
geom_label(aes(label=response_no), nudge_x=0.2, data=subset(nulls, perc > 0.15))

@
\caption{Empirical probabilities $p_i$ for each lineup. All lineups have some null plots that are high in the number of picks.}
\end{figure}




\bibliographystyle{asa}
\bibliography{references}

\end{document}
